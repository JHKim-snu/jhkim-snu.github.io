---
permalink: /
author_profile: true
title: "About me"
excerpt: "About me"
redirect_from: 
  - /about/
  - /about.html
---
I am a 2nd year master student in Cognitive Science Program at <a href="http://en.snu.ac.kr">Seoul National University</a>, advised by <a href="https://bi.snu.ac.kr/~btzhang/">Prof. Byoung-Tak Zhang</a>. My research interests mainly span in computer vision, natural language processing, and machine learning. I'm particularly interested in the interplay between vision and language as <q>I firmly believe that the vision and language have complementary strengths</q>. The long-term goal of my research is building AI agents that can see everyday scenes and fluently communicate with people. Prior to joining graduate school, I received my Bachelor's degree in software and computer science from <a href="http://www.ajou.ac.kr/en/">Ajou University</a>.

## Recent News
<span style="color:#ff7272;"><b>NEW!</b></span> [January 2020] Our paper has been accepted to ICASSP 2020!<br>
<span style="color:#ff7272;"><b>NEW!</b></span> [December 2019] From January, I'll be a research intern at <a href="https://www.skt.ai">SK T-Brain</a> !<br>
<span style="color:#ff7272;"><b>NEW!</b></span> [November 2019] I gave a spotlight talk at <a href="https://videoturingtest.github.io">Video Turing Test workshop</a>, ICCV 2019.
<details>
  <summary>show more</summary>
  <span style="color:#ff7272;"><b>NEW!</b></span> [October 2019] I gave an invited talk at <a href="https://www.skt.ai">SK Telecom AI Center</a>.<br>
<span style="color:#ff7272"><b>NEW!</b></span> [August 2019] Excited to announce that <a href="https://arxiv.org/abs/1902.09368">our paper</a> has been accepted to <a href="https://www.emnlp-ijcnlp2019.org/">EMNLP 2019</a>.<br>
  <span style="color:#ff7272;"><b>NEW!</b></span> [June 2019] Our proposed method ranks <b>3rd place</b> in <a href="https://visualdialog.org/challenge/2019">Visual Dialog Challenge 2019</a>!!<br>
  <span style="color:#ff7272;"><b>NEW!</b></span> [August 2018] We have a paper accepted to ECCV 2018 Workshop on <a href="http://vizwiz.org/workshop/">VizWiz Grand Challenge</a>.
</details>

## Publications
<table align="center" style="border-collapse: collapse; border: none;" >
    <!-- Contextualized Bilinear Attention Networks -->
    <tr style="border: none;">
        <td align="center" style="border: none;"><img src="../images/LPART-20.png?raw=true" alt="Photo" width="270" height="135" /></td>
        <td align="left" style="border: none;"><b><span style="font-size: 15px;">Label Propagation Adaptive Resonance Theory for Semi-Supervised Continuous Learning</span></b><br>
          <span style="font-size:15px;">Taehyeong Kim, Injune Hwang, <u>Gi-Cheon Kang</u>, Won-Seok Choi, Hyunseo Kim, Byoung-Tak Zhang</span><br>
          <span style="font-size:15px;"><i>ICASSP 2020</i></span><br>
          </td> 
    </tr>  
    <tr style="border: none;">
        <td style="border: none;" colspan="3"><hr style="border: dashed 1px #8c8b8b;"></td>
    </tr>  
    <!-- Dual Attention Networks -->
    <tr style="border: none;">
        <td align="center" style="border: none;"><img src="../images/DAN-19.png?raw=true" alt="Photo" width="270" height="135" /></td>
        <td align="left" style="border: none;"><b><span style="font-size: 15px;">Dual Attention Networks for Visual Reference Resolution in Visual Dialog</span></b><br>
          <span style="font-size:15px;"><u>Gi-Cheon Kang</u>, Jaeseo Lim, Byoung-Tak Zhang</span><br>
          <span style="font-size:15px;"><i>EMNLP 2019</i></span><br>
          <span style="font-size:15px;">(Ranked 3<sup>rd</sup> in <a href="https://visualdialog.org/challenge/2019">Visual Dialog Challenge 2019</a>)</span><br>
          <span style="font-size:15px;"><a class="btn btn--info" href="https://arxiv.org/pdf/1902.09368.pdf">Paper</a></span>
          <span style="font-size:15px;"><a class="btn btn--success" href="https://github.com/gicheonkang/DAN-VisDial">Code</a> </span>
          <span style="font-size:15px;"><a class="btn btn--inverse" href="https://docs.google.com/viewer?url=https://raw.githubusercontent.com/gicheonkang/gicheonkang.github.io/master/files/DAN-19-slide.pdf">Slides</a></span>
        </td>
    </tr>
    <tr style="border: none;">
        <td style="border: none;" colspan="3"><hr style="border: dashed 1px #8c8b8b;"></td>
    </tr>
    <!-- Contextualized Bilinear Attention Networks -->
    <tr style="border: none;">
        <td align="center" style="border: none;"><img src="../images/CBAN-18.png?raw=true" alt="Photo" width="270" height="135" /></td>
        <td align="left" style="border: none;"><b><span style="font-size: 15px;">Contextualized Bilinear Attention Networks</span></b><br>
          <span style="font-size:15px;"><u>Gi-Cheon Kang</u>, Seonil Son, Byoung-Tak Zhang</span><br>
          <span style="font-size:15px;"><i>ECCV 2018 VizWiz Workshop</i></span><br>
          <span style="font-size:15px;"><a class="btn btn--info" href="https://bi.snu.ac.kr/Publications/Conferences/International/ECCV2018_Workshop_VizWiz_GCKang.pdf">Paper</a></span>
          </td> 
    </tr>
</table>

## Talks
- Dual Attention Networks for Visual Reference Resolution in Visual Dialog <br>
<span style="font-size:16px;">Video Turing Test Workshop, ICCV 2019</span><br>
<span style="font-size:16px;">T.TOC, SK Telecom AI Center</span><br>

## Side Projects
<script async defer src="https://buttons.github.io/buttons.js"></script>
<table align="center" style="border-collapse: collapse; border: none;" >
    <tr style="border: none;">
        <td align="center" style="border: none;"><img src="../images/fast-face-android.png?raw=true" alt="Photo" width="270" /></td>
        <td style="border: none;"></td>
      <td style="border: none;"><b><span style="font-size: 15px;">Fast-Face Android &nbsp; </span></b><a class="github-button" href="https://github.com/gicheonkang/fast-face-android" data-icon="octicon-star" data-show-count="true" aria-label="Star gicheonkang/fast-face-android on GitHub">Star</a><br>
          <span style="font-size:15px;">Fast Face is an android application which detects facial landmark. It detects 68 landmarks of human face chin to eyebrow in real-time. Also, it can detect people up to 3 if you guys show your frontal faces.</span><br>
        </td>
    </tr>    
</table>

## Affiliations
![ajou](/images/ajou.png){: width="90" height="90"} &nbsp; ![snu](/images/snu.png){: width="70" height="70"} &nbsp;&nbsp; ![skt](/images/skt.png){: width="130" height="80"}


<style>
  @media screen and (max-width: 750px) {
  table thead {
    border: none;
    clip: rect(0 0 0 0);
    height: 1px;
    margin: -1px;
    overflow: hidden;
    padding: 0;
    position: absolute;
    width: 1px;
  }
  
  table tr {
    border-bottom: 3px solid #ddd;
    display: block;
  }
  
  table td {
    border-bottom: 1px solid #ddd;
    display: block;
    text-align: left;
  }
  
  table td::before {
    content: attr(data-label);
    float: left;
  }
}

</style>

