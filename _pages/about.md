---
permalink: /
author_profile: true
title: "About me"
excerpt: "About me"
redirect_from: 
  - /about/
  - /about.html
---
I am a 2nd year master student in Cognitive Science Program at <a href="http://en.snu.ac.kr">Seoul National University</a>, advised by <a href="https://bi.snu.ac.kr/~btzhang/">Prof. Byoung-Tak Zhang</a>. My research interests mainly span in computer vision and natural language processing, and machine learning. I'm particularly interested in machine learning and its application in building agents that can see everyday scenes and fluently communicate with people. <br>
Prior to joining graduate school, I received my Bachelor's degree in software and computer science from <a href="http://www.ajou.ac.kr/en/">Ajou University</a>. 
<br>

## Recent News
<span style="color:#ff7272;"><b>NEW!</b></span> [October 2019] I gave a spotlight talk at <a href="https://videoturingtest.github.io">Video Turing Test workshop</a>, ICCV 2019<br>
<span style="color:#ff7272;"><b>NEW!</b></span> [October 2019] I gave an invited talk at <a href="https://www.skt.ai">SK Telecom AI Center</a>.<br>
<span style="color:#ff7272"><b>NEW!</b></span> [August 2019] Excited to announce that <a href="https://arxiv.org/abs/1902.09368">our paper</a> has been accepted to <a href="https://www.emnlp-ijcnlp2019.org/">EMNLP 2019</a>.
<details>
  <summary>show more</summary>
  <span style="color:#ff7272;"><b>NEW!</b></span> [June 2019] Our proposed method ranks <b>3rd place</b> in <a href="https://visualdialog.org/challenge/2019">Visual Dialog Challenge 2019</a>!!<br>
  <span style="color:#ff7272;"><b>NEW!</b></span> [August 2018] We have a paper accepted to ECCV 2018 Workshop on <a href="http://vizwiz.org/workshop/">VizWiz Grand Challenge</a>.
</details>

## Publications
<table align="center" style="border-collapse: collapse; border: none;" >
    <!-- Dual Attention Networks -->
    <tr style="border: none;">
        <td align="center" style="border: none;"><img src="https://github.com/gicheonkang/gicheonkang.github.io/blob/master/images/DAN-19.png?raw=true" alt="Photo" width="300" height="150" /></td>
        <td align="left" style="border: none;"><b><span style="font-size: 16px;">Dual Attention Networks for Visual Reference Resolution in Visual Dialog</span></b><br>
          <span style="font-size:15px;"><u>G.-C. Kang</u>, J. Lim, and B.-T. Zhang</span><br>
          <span style="font-size:15px;"><i>Conference on Empirical Methods in Natural Language Processing (EMNLP)</i>, 2019.</span><br>
          <span style="font-size:15px;">(Ranked 3<sup>rd</sup> place in <a href="https://visualdialog.org/challenge/2019">Visual Dialog Challenge 2019</a>)</span><br>
          <span style="font-size:15px;"><a class="btn btn--info" href="https://arxiv.org/pdf/1902.09368.pdf">Paper</a></span>
          <span style="font-size:15px;"><a class="btn btn--success" href="https://github.com/gicheonkang/DAN-VisDial">Code</a> </span>
          <span style="font-size:15px;"><a class="btn btn--inverse" href="https://docs.google.com/viewer?url=https://raw.githubusercontent.com/gicheonkang/gicheonkang.github.io/master/files/DAN-19-slide.pdf">Slides</a></span>
        </td>
    </tr>
    <!-- CBAN -->
    <tr style="border: none;">
        <td style="border: none;" colspan="3"><hr style="border: dashed 1px #8c8b8b;"></td>
    </tr>
    <!-- Contextualized Bilinear Attention Networks -->
    <tr style="border: none;">
        <td align="center" style="border: none;"><img src="https://github.com/gicheonkang/gicheonkang.github.io/blob/master/images/CBAN-18.png?raw=true" alt="Photo" width="300" height="150" /></td>
        <td align="left" style="border: none;"><b><span style="font-size: 16px;">Contextualized Bilinear Attention Networks</span></b><br>
          <span style="font-size:15px;"><u>G.-C. Kang</u>, S. Son, and B.-T. Zhang</span><br>
          <span style="font-size:15px;"><i>European Conference on Computer Vision VizWiz Workshop (ECCVW)</i>, 2018</span><br>
          <span style="font-size:15px;"><a class="btn btn--info" href="https://bi.snu.ac.kr/Publications/Conferences/International/ECCV2018_Workshop_VizWiz_GCKang.pdf">Paper</a></span>
          </td> 
    </tr>
</table>

## Talks
<ul>
  <li>Dual Attention Nets for Visual Reference Resolution in VisDial, SK Telecom AI Center, 2019.</li>
</ul>

## Side Projects
<script async defer src="https://buttons.github.io/buttons.js"></script>
<table align="center" style="border-collapse: collapse; border: none;" >
    <!-- Dual Attention Networks -->
    <tr style="border: none;">
        <td align="center" style="border: none;"><img src="https://github.com/gicheonkang/gicheonkang.github.io/blob/master/images/fast-face-android.png?raw=true" alt="Photo" width="300" /></td>
        <td style="border: none;"></td>
        <td style="border: none;"><b><span style="font-size: 16px;">Fast-Face Android &nbsp; </span></b><a class="github-button" href="https://github.com/gicheonkang/fast-face-android" data-icon="octicon-star" data-show-count="true" aria-label="Star gicheonkang/fast-face-android on GitHub">Star</a><br>
          Fast Face is an android application which detects facial landmark. It detects 68 landmarks of human face chin to eyebrow in real-time. Also, it can detect people up to 3 if you guys show your frontal faces.<br>
        </td>
    </tr>    
</table>

<style>
  @media screen and (max-width: 750px) {
  table thead {
    border: none;
    clip: rect(0 0 0 0);
    height: 1px;
    margin: -1px;
    overflow: hidden;
    padding: 0;
    position: absolute;
    width: 1px;
  }
  
  table tr {
    border-bottom: 3px solid #ddd;
    display: block;
  }
  
  table td {
    border-bottom: 1px solid #ddd;
    display: block;
    text-align: left;
  }
  
  table td::before {
    content: attr(data-label);
    float: left;
  }
}
</style>

---
[1]: http://en.snu.ac.kr
[2]: https://bi.snu.ac.kr/~btzhang/
[3]: http://www.ajou.ac.kr/en/


