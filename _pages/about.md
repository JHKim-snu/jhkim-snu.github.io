---
permalink: /
author_profile: true
title: "About"
excerpt: "About me"
redirect_from: 
  - /about/
  - /about.html
---
I am a 2nd-year Ph.D. student in the Graduate School of AI at <a href="http://en.snu.ac.kr">Seoul National University</a>, advised by <a href="https://bi.snu.ac.kr/~btzhang/">Prof. Byoung-Tak Zhang</a>. My research straddles machine learning, natural language processing, and computer vision. I'm particularly interested in <a href="https://gicheonkang.com" style="pointer-events: none">grounded language learning</a> which aims to connect language to non-linguistic experiences like sensory perception and action. The long-term goal of my research is to build machines that can (i) perceive their everyday surroundings, (ii) communicate with human via natural language, and (iii) make reliable decisions using language, vision, sound, etc.    

Prior to joining Ph.D. program, I did my master study in Cognitive Science at <a href="http://en.snu.ac.kr">Seoul National University</a>. I earned my Bachelor's degree in Computer Science from <a href="http://www.ajou.ac.kr/en/">Ajou University</a>.


## Recent News
<span style="color:#ff7272;"><b>NEW!</b></span> [October 2021] One paper is accepted to NeurIPS 2021 CtrlGen Workshop. <br> 
<span style="color:#ff7272;"><b>NEW!</b></span> [August 2021] One paper is accepted to Findings of EMNLP 2021.

<details>
  <summary>show more</summary>
  <span style="color:#ff7272;"><b>NEW!</b></span> [May 2021] One paper is accepted to ACL 2021. <br>
  <span style="color:#ff7272;"><b>NEW!</b></span> [September 2020] I'm starting my Ph.D. in this fall. <br>
  <span style="color:#ff7272;"><b>NEW!</b></span> [June 2020] From July, I'll join <a href="https://aiis.snu.ac.kr">SNU AI Institute</a> (AIIS) as a researcher. <br>
  <span style="color:#ff7272;"><b>NEW!</b></span> [January 2020] Our paper has been accepted to ICASSP 2020!<br>
  <span style="color:#ff7272;"><b>NEW!</b></span> [December 2019] From January, I'll be a research intern at <a href="https://www.skt.ai">SK T-Brain</a>!<br>
  <span style="color:#ff7272;"><b>NEW!</b></span> [November 2019] I gave a spotlight talk at <a href="https://videoturingtest.github.io">Video Turing Test workshop</a>, ICCV 2019.<br>
  <span style="color:#ff7272;"><b>NEW!</b></span> [October 2019] I gave an invited talk at <a href="https://www.skt.ai">SK Telecom AI Center</a>.<br>
<span style="color:#ff7272"><b>NEW!</b></span> [August 2019] Excited to announce that <a href="https://arxiv.org/abs/1902.09368">our paper</a> has been accepted to <a href="https://www.emnlp-ijcnlp2019.org/">EMNLP 2019</a>.<br>
  <span style="color:#ff7272;"><b>NEW!</b></span> [June 2019] Our proposed method ranks <b>3rd place</b> in <a href="https://visualdialog.org/challenge/2019">Visual Dialog Challenge 2019</a>!!<br>
  <span style="color:#ff7272;"><b>NEW!</b></span> [August 2018] We have a paper accepted to ECCV 2018 Workshop on <a href="http://vizwiz.org/workshop/">VizWiz Grand Challenge</a>.
</details>

## Publications
<table align="center" style="border-collapse: collapse; border: none;">
    <!-- C3 -->
    <tr style="border: none;">
        <td align="center" style="border: none;"><img src="../images/C3-21.png?raw=true" alt="Photo" width="280" height="140" /></td>
      <td align="left" style="border: none;"><span style="font-size: 15px;"><b>C<sup>3</sup>: Contrastive Learning for Cross-domain Correspondence in Few-shot Image Generation</b></span><br>
          <span style="font-size:13px; color:#0000008f;">Hyukgi Lee, <u style="text-decoration-color: #0000008f;">Gi-Cheon Kang</u>, Chang-Hoon Jeong, Hanwool Sul, Byoung-Tak Zhang</span><br>
          <span style="font-size:13px; color:#0275d8;"><b>NeurIPS 2021 CtrlGen Workshop</b></span><br>
          <span style="font-size:13px;"><a class="btn btn--inverse" href="https://ctrlgenworkshop.github.io">Workshop</a></span>
          </td> 
    </tr>
    <tr style="border: none;">
        <td style="border: none;" colspan="3"><hr style="border: solid 0.3px #EDEDED;"></td>
    </tr>
    <!-- DialGraph -->
    <tr style="border: none;">
        <td align="center" style="border: none;"><img src="../images/SGLN-20.png?raw=true" alt="Photo" width="280" height="140" /></td>
        <td align="left" style="border: none;"><span style="font-size: 15px;"><b>Reasoning Visual Dialog with Sparse Graph Learning and Knowledge Transfer</b></span><br>
          <span style="font-size:13px; color:#0000008f;"><u style="text-decoration-color: #0000008f;">Gi-Cheon Kang</u>, Junseok Park, Hwaran Lee, Byoung-Tak Zhang<sup>*</sup>, Jin-Hwa Kim<sup>*</sup></span><br>
          <span style="font-size:13px; color:#0275d8;"><b>EMNLP 2021 Findings</b></span><br>
          <span style="font-size:13px;"><a class="btn btn--inverse" href="https://arxiv.org/abs/2004.06698">Paper</a></span>
          <span style="font-size:13px;"><a class="btn btn--inverse" href="https://github.com/gicheonkang/sglkt-visdial">Code</a></span>
          <span style="font-size:13px;"><a class="btn btn--inverse" href="https://docs.google.com/viewer?url=https://raw.githubusercontent.com/gicheonkang/gicheonkang.github.io/master/files/SGLKT-21-slide.pdf">Slides</a></span>
          </td> 
    </tr>  
    <tr style="border: none;">
        <td style="border: none;" colspan="3"><hr style="border: solid 0.3px #EDEDED;"></td>
    </tr> 
    <!-- MASN -->
    <tr style="border: none;">
        <td align="center" style="border: none;"><img src="../images/MASN-21.jpeg?raw=true" alt="Photo" width="280" height="140" /></td>
        <td align="left" style="border: none;"><span style="font-size: 15px;"><b>Attend What You Need: Motion-Appearance Synergistic Networks for Video Question Answering</b></span><br>
          <span style="font-size:13px; color:#0000008f;">Ahjeong Seo, <u style="text-decoration-color: #0000008f;">Gi-Cheon Kang</u>, Joonhan Park, Byoung-Tak Zhang</span><br>
          <span style="font-size:13px; color:#0275d8;"><b>ACL 2021</b></span><br>
          <span style="font-size:13px;"><a class="btn btn--inverse" href="https://aclanthology.org/2021.acl-long.481">Paper</a></span>
          <span style="font-size:13px;"><a class="btn btn--inverse" href="https://github.com/ahjeongseo/MASN-pytorch">Code</a></span>
          </td> 
    </tr>  
    <tr style="border: none;">
        <td style="border: none;" colspan="3"><hr style="border: solid 0.3px #EDEDED;"></td>
    </tr>
    <!-- LPART -->
    <tr style="border: none;">
        <td align="center" style="border: none;"><img src="../images/LPART-20.png?raw=true" alt="Photo" width="280" height="140" /></td>
        <td align="left" style="border: none;"><span style="font-size: 15px;"><b>Label Propagation Adaptive Resonance Theory for Semi-Supervised Continuous Learning</b></span><br>
          <span style="font-size:13px; color:#0000008f;">Taehyeong Kim, Injune Hwang, <u style="text-decoration-color: #0000008f;">Gi-Cheon Kang</u>, Won-Seok Choi, Hyunseo Kim, Byoung-Tak Zhang</span><br>
          <span style="font-size:13px; color:#0275d8;"><b>ICASSP 2020</b></span><br>
          <span style="font-size:13px;"><a class="btn btn--inverse" href="https://ieeexplore.ieee.org/document/9054655">Paper</a></span>
          </td> 
    </tr>  
    <tr style="border: none;">
        <td style="border: none;" colspan="3"><hr style="border: solid 0.3px #EDEDED;"></td>
    </tr>  
    <!-- Dual Attention Networks -->
    <tr style="border: none;">
        <td align="center" style="border: none;"><img src="../images/DAN-19.png?raw=true" alt="Photo" width="280" height="140" /></td>
        <td align="left" style="border: none;"><span style="font-size: 15px;"><b>Dual Attention Networks for Visual Reference Resolution in Visual Dialog</b></span><br>
          <span style="font-size:13px; color:#0000008f;"><u style="text-decoration-color: #0000008f;">Gi-Cheon Kang</u>, Jaeseo Lim, Byoung-Tak Zhang</span><br>
          <span style="font-size:13px; color:#0275d8;"><b>EMNLP 2019</b></span><br>
          <span style="font-size:13px; color:#0275d8;"><b>Ranked 3rd Place in VisDial Challenge 2019</b></span><br>
          <span style="font-size:13px;"><a class="btn btn--inverse" href="https://www.aclweb.org/anthology/D19-1209/">Paper</a></span>
          <span style="font-size:13px;"><a class="btn btn--inverse" href="https://github.com/gicheonkang/DAN-VisDial">Code</a></span>
          <span style="font-size:13px;"><a class="btn btn--inverse" href="https://docs.google.com/viewer?url=https://raw.githubusercontent.com/gicheonkang/gicheonkang.github.io/master/files/DAN-19-slide.pdf">Slides</a></span>
        </td>
    </tr>
    <tr style="border: none;">
        <td style="border: none;" colspan="3"><hr style="border: solid 0.3px #EDEDED;"></td>
    </tr>
    <!-- Contextualized Bilinear Attention Networks -->
    <tr style="border: none;">
        <td align="center" style="border: none;"><img src="../images/CBAN-18.png?raw=true" alt="Photo" width="280" height="140" /></td>
        <td align="left" style="border: none;"><span style="font-size: 15px;"><b>Contextualized Bilinear Attention Networks</b></span><br>
          <span style="font-size:13px; color:#0000008f;"><u style="text-decoration-color: #0000008f;">Gi-Cheon Kang</u>, Seonil Son, Byoung-Tak Zhang</span><br>
          <span style="font-size:13px; color:#0275d8;"><b>ECCV Workshop on VizWiz Challenge 2018</b></span><br>
          <span style="font-size:13px;"><a class="btn btn--inverse" href="https://bi.snu.ac.kr/Publications/Conferences/International/ECCV2018_Workshop_VizWiz_GCKang.pdf">Paper</a></span>
          </td> 
    </tr>
</table>

## Invited Talks
- Reasoning Visual Dialog with Sparse Graph Learning and Knowledge Transfer <br>
<span style="font-size:13px;"> - Annual Conference on Human and Cognitive Language Technology </span><br>
- Dual Attention Networks for Visual Reference Resolution in Visual Dialog <br>
<span style="font-size:13px;"> - Video Turing Test Workshop, ICCV 2019 (Spotlight Talk)</span><br>
<span style="font-size:13px;"> - T.TOC, SK Telecom AI Center</span><br>

## Software
<script async defer src="https://buttons.github.io/buttons.js"></script>
<table align="center" style="border-collapse: collapse; border: none;" >
    <tr style="border: none;">
        <td align="center" style="border: none;"><img src="../images/fast-face-android.png?raw=true" alt="Photo" width="270" /></td>
      <td align="left" style="border: none;"><b><span style="font-size: 16px;">Fast-Face Android &nbsp; </span></b><a class="github-button" href="https://github.com/gicheonkang/fast-face-android" data-icon="octicon-star" data-show-count="true" aria-label="Star gicheonkang/fast-face-android on GitHub">Star</a><br>
          <span style="font-size:13px;">Fast Face is an android application which detects facial landmark. It detects 68 landmarks of human face chin to eyebrow in real-time. Also, it can detect people up to 3 if you guys show your frontal faces.</span><br>
        </td>
    </tr>    
</table>

## Affiliations
![ajou](/images/ajou.png){: width="90" height="90"} &nbsp; ![snu](/images/snu.png){: width="70" height="70"} &nbsp;&nbsp;&nbsp; ![aiis](/images/aiis.png){: width="130" height="55"} <br>

&nbsp;&nbsp;&nbsp; ![skt](/images/skt.png){: width="130" height="80"} 


<style>
  @media screen and (max-width: 750px) {
  table thead {
    border: none;
    clip: rect(0 0 0 0);
    height: 1px;
    margin: -1px;
    overflow: hidden;
    padding: 0;
    position: absolute;
    width: 1px;
  }
  
  table tr {
    border-bottom: 3px solid #ddd;
    display: block;
  }
  
  table td {
    border-bottom: 1px solid #ddd;
    display: block;
    text-align: left;
  }
  
  table td::before {
    content: attr(data-label);
    float: left;
  }
}

</style>

