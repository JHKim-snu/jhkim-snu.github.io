---
permalink: /
author_profile: true
title: "About me"
excerpt: "About me"
redirect_from: 
  - /about/
  - /about.html
---
I am a 1st year Ph.D student in Artificial Intelligence (AI) program at <a href="http://en.snu.ac.kr">Seoul National University</a>, advised by <a href="https://bi.snu.ac.kr/~btzhang/">Prof. Byoung-Tak Zhang</a>. My research interests mainly span in the area of machine learning, natural language processing, and computer vision. Specifically, I've studied <a href="https://gicheonkang.com" style="pointer-events: none">grounded language learning</a> which aims to connect language to perception (<i>e.g.,</i> vision) and action (<i>e.g.,</i> navigation). The long-term goal of my research is to build AI agents that can (1) perceive their everyday surroundings, (2) communicate with human via natural language, and (3) make reliable decisions using language, vision, and other information.    

Prior to joining Ph.D program, I did my master study in Cognitive Science at <a href="http://en.snu.ac.kr">Seoul National University</a>. I earned my Bachelor's degree in Computer Science from <a href="http://www.ajou.ac.kr/en/">Ajou University</a>.


## Recent News
<span style="color:#ff7272;"><b>NEW!</b></span> [September 2020] I'm starting my Ph.D. in this fall. 
<details>
  <summary>show more</summary>
  <span style="color:#ff7272;"><b>NEW!</b></span> [June 2020] From July, I'll join <a href="https://aiis.snu.ac.kr">SNU AI Institute</a> (AIIS) as a researcher. <br>
  <span style="color:#ff7272;"><b>NEW!</b></span> [January 2020] Our paper has been accepted to ICASSP 2020!<br>
  <span style="color:#ff7272;"><b>NEW!</b></span> [December 2019] From January, I'll be a research intern at <a href="https://www.skt.ai">SK T-Brain</a>!<br>
  <span style="color:#ff7272;"><b>NEW!</b></span> [November 2019] I gave a spotlight talk at <a href="https://videoturingtest.github.io">Video Turing Test workshop</a>, ICCV 2019.<br>
  <span style="color:#ff7272;"><b>NEW!</b></span> [October 2019] I gave an invited talk at <a href="https://www.skt.ai">SK Telecom AI Center</a>.<br>
<span style="color:#ff7272"><b>NEW!</b></span> [August 2019] Excited to announce that <a href="https://arxiv.org/abs/1902.09368">our paper</a> has been accepted to <a href="https://www.emnlp-ijcnlp2019.org/">EMNLP 2019</a>.<br>
  <span style="color:#ff7272;"><b>NEW!</b></span> [June 2019] Our proposed method ranks <b>3rd place</b> in <a href="https://visualdialog.org/challenge/2019">Visual Dialog Challenge 2019</a>!!<br>
  <span style="color:#ff7272;"><b>NEW!</b></span> [August 2018] We have a paper accepted to ECCV 2018 Workshop on <a href="http://vizwiz.org/workshop/">VizWiz Grand Challenge</a>.
</details>

## Publications
<table align="center" style="border-collapse: collapse; border: none;">
    <!-- Sparse Graph Learning Network -->
    <tr style="border: none;">
        <td align="center" style="border: none;"><img src="../images/SGLN-20.png?raw=true" alt="Photo" width="280" height="140" /></td>
        <td align="left" style="border: none;"><span style="font-size: 16px;"><b>DialGraph: Sparse Graph Learning Networks for Visual Dialog</b></span><br>
          <span style="font-size:14px; color:#0000008f;">Gi-Cheon Kang, Junseok Park, Hwaran Lee, Byoung-Tak Zhang, Jin-Hwa Kim</span><br>
          <span style="font-size:14px;"><a class="btn btn--inverse" href="https://arxiv.org/abs/2004.06698">Paper</a></span>
          </td> 
    </tr>  
    <tr style="border: none;">
        <td style="border: none;" colspan="3"><hr style="border: solid 0.3px #EDEDED;"></td>
    </tr>
    <!-- LPART -->
    <tr style="border: none;">
        <td align="center" style="border: none;"><img src="../images/LPART-20.png?raw=true" alt="Photo" width="280" height="140" /></td>
        <td align="left" style="border: none;"><span style="font-size: 16px;"><b>Label Propagation Adaptive Resonance Theory for Semi-Supervised Continuous Learning</b></span><br>
          <span style="font-size:14px; color:#0000008f;">Taehyeong Kim, Injune Hwang, Gi-Cheon Kang, Won-Seok Choi, Hyunseo Kim, Byoung-Tak Zhang</span><br>
          <span style="font-size:14px; color:#0275d8;"><b>ICASSP 2020</b></span><br>
          <span style="font-size:14px;"><a class="btn btn--inverse" href="https://ieeexplore.ieee.org/document/9054655">Paper</a></span>
          </td> 
    </tr>  
    <tr style="border: none;">
        <td style="border: none;" colspan="3"><hr style="border: solid 0.3px #EDEDED;"></td>
    </tr>  
    <!-- Dual Attention Networks -->
    <tr style="border: none;">
        <td align="center" style="border: none;"><img src="../images/DAN-19.png?raw=true" alt="Photo" width="280" height="140" /></td>
        <td align="left" style="border: none;"><span style="font-size: 16px;"><b>Dual Attention Networks for Visual Reference Resolution in Visual Dialog</b></span><br>
          <span style="font-size:14px; color:#0000008f;">Gi-Cheon Kang, Jaeseo Lim, Byoung-Tak Zhang</span><br>
          <span style="font-size:14px; color:#0275d8;"><b>EMNLP 2019</b></span><br>
          <span style="font-size:14px;"><a class="btn btn--inverse" href="https://www.aclweb.org/anthology/D19-1209/">Paper</a></span>
          <span style="font-size:14px;"><a class="btn btn--inverse" href="https://github.com/gicheonkang/DAN-VisDial">Code</a></span>
          <span style="font-size:14px;"><a class="btn btn--inverse" href="https://docs.google.com/viewer?url=https://raw.githubusercontent.com/gicheonkang/gicheonkang.github.io/master/files/DAN-19-slide.pdf">Slides</a></span>
        </td>
    </tr>
    <tr style="border: none;">
        <td style="border: none;" colspan="3"><hr style="border: solid 0.3px #EDEDED;"></td>
    </tr>
    <!-- Contextualized Bilinear Attention Networks -->
    <tr style="border: none;">
        <td align="center" style="border: none;"><img src="../images/CBAN-18.png?raw=true" alt="Photo" width="280" height="140" /></td>
        <td align="left" style="border: none;"><span style="font-size: 16px;"><b>Contextualized Bilinear Attention Networks</b></span><br>
          <span style="font-size:14px; color:#0000008f;">Gi-Cheon Kang, Seonil Son, Byoung-Tak Zhang</span><br>
          <span style="font-size:14px; color:#0275d8;"><b>ECCV Workshop on VizWiz Challenge 2018</b></span><br>
          <span style="font-size:14px;"><a class="btn btn--inverse" href="https://bi.snu.ac.kr/Publications/Conferences/International/ECCV2018_Workshop_VizWiz_GCKang.pdf">Paper</a></span>
          </td> 
    </tr>
</table>

## Talks
- Dual Attention Networks for Visual Reference Resolution in Visual Dialog <br>
<span style="font-size:15px;"> - Video Turing Test Workshop, ICCV 2019 (Spotlight Talk)</span><br>
<span style="font-size:15px;"> - T.TOC, SK Telecom AI Center</span><br>

## Software
<script async defer src="https://buttons.github.io/buttons.js"></script>
<table align="center" style="border-collapse: collapse; border: none;" >
    <tr style="border: none;">
        <td align="center" style="border: none;"><img src="../images/fast-face-android.png?raw=true" alt="Photo" width="270" /></td>
      <td align="left" style="border: none;"><b><span style="font-size: 16px;">Fast-Face Android &nbsp; </span></b><a class="github-button" href="https://github.com/gicheonkang/fast-face-android" data-icon="octicon-star" data-show-count="true" aria-label="Star gicheonkang/fast-face-android on GitHub">Star</a><br>
          <span style="font-size:15px;">Fast Face is an android application which detects facial landmark. It detects 68 landmarks of human face chin to eyebrow in real-time. Also, it can detect people up to 3 if you guys show your frontal faces.</span><br>
        </td>
    </tr>    
</table>

## Affiliations
![ajou](/images/ajou.png){: width="90" height="90"} &nbsp; ![snu](/images/snu.png){: width="70" height="70"} &nbsp;&nbsp;&nbsp; ![aiis](/images/aiis.png){: width="130" height="55"} <br>

&nbsp;&nbsp;&nbsp; ![skt](/images/skt.png){: width="130" height="80"} 


<style>
  @media screen and (max-width: 750px) {
  table thead {
    border: none;
    clip: rect(0 0 0 0);
    height: 1px;
    margin: -1px;
    overflow: hidden;
    padding: 0;
    position: absolute;
    width: 1px;
  }
  
  table tr {
    border-bottom: 3px solid #ddd;
    display: block;
  }
  
  table td {
    border-bottom: 1px solid #ddd;
    display: block;
    text-align: left;
  }
  
  table td::before {
    content: attr(data-label);
    float: left;
  }
}

</style>

