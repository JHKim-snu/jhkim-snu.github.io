---
layout:     page
title:
permalink:  /
---

<div class="row">
    <div class="col-sm-6 col-xs-12">
        <img src="/img/profile_new.jpg">
    </div>
    <div class="col-sm-6 col-xs-12" style="margin-bottom: 0;">
        Ph.D. Student<br>
        Graduate School of AI<br>
        Seoul National University<br>
        <a target="_blank" href="mailto:gchnkang@gmail.com">gchnkang at gmail dot com</a>
    </div>
</div>
<hr>


<a name="/bio"></a>

# Bio

I am a third year Ph.D. student in the Graduate School of AI at [Seoul National University][1], and a researcher at [AIIS][2], advised by [Prof. Byoung-Tak Zhang][3]. My research straddles machine learning, natural language processing, and computer vision. I'm particularly interested in <span class="fw-550">grounded language learning</span> which aims to connect language to non-linguistic experiences like sensory perception and action. The long-term goal of my research is to build machines that can (i) perceive their everyday surroundings, (ii) communicate with humans via natural language, and (iii) make reliable decisions using language, vision, etc.    

Prior to joining Ph.D. program, I did my master study in Cognitive Science at [Seoul National University][1]. Studying cognitive science has had a great influence on my research. I earned my Bachelor's degree in Computer Science from [Ajou University][5].

---

<a name="/news"></a>

# News

- [Jun 2023] One paper is accepted to <a href="https://ieee-iros.org">IROS 2023</a>!
- [Mar 2023] Happy to announce that <a href="https://arxiv.org/abs/2205.12502">our paper</a> is accepted to <a href="https://cvpr2023.thecvf.com">CVPR 2023</a>!
- [Jun 2022] One paper is accepted to ICML 2022 <a href="[https://sites.google.com/nycu.edu.tw/hcis/home](https://pretraining.github.io)">Pre-training Workshop</a>.
- [May 2022] Thrilled to announce that our <a href="https://arxiv.org/abs/2205.12502">new preprint</a> is released!
- [Apr 2022] One paper is accepted to CVPR 2022 <a href="https://sites.google.com/nycu.edu.tw/hcis/home">HCIS Workshop</a>.
- [Dec 2021] I gave an invited talk at Korea Software Congress.
- [Oct 2021] One paper is accepted to NeurIPS 2021 CtrlGen Workshop.
- [Aug 2021] One paper is accepted to Findings of EMNLP 2021.
- [May 2021] One paper is accepted to ACL 2021.
- [Sep 2020] I'm starting my Ph.D. in this fall.
- [Jun 2020] From July, I'll join <a href="https://aiis.snu.ac.kr">SNU AI Institute</a> (AIIS) as a researcher.
- [Jan 2020] Our paper has been accepted to ICASSP 2020!
- [Dec 2019] From January, I'll be a research intern at <a href="https://www.skt.ai">SK T-Brain</a>!
- [Nov 2019] I gave a spotlight talk at <a href="https://videoturingtest.github.io">Video Turing Test workshop</a>, ICCV 2019.
- [Oct 2019] I gave an invited talk at <a href="https://www.skt.ai">SK Telecom AI Center</a>.
- [Aug 2019] Excited to announce that <a href="https://arxiv.org/abs/1902.09368">our paper</a> has been accepted to <a href="https://www.emnlp-ijcnlp2019.org/">EMNLP 2019</a>.
- [Jun 2019] Our proposed method ranks <b>3rd place</b> in <a href="https://visualdialog.org/challenge/2019">Visual Dialog Challenge 2019</a>!!
- [Aug 2018] We have a paper accepted to ECCV 2018 Workshop on <a href="http://vizwiz.org/workshop/">VizWiz Grand Challenge</a>.

<div id="read-more-button">
    <a nohref>Read more</a>
</div>

<hr>


<a name="/publications"></a>

# Publications

<a name="/gvcci"></a>
<h2 class="pubt">GVCCI: Lifelong Learning of Visual Grounding for Language-Guided Robotic Manipulation</h2>
<p class="pubd">
    <span class="authors">Junghyun Kim, <span class="u">Gi-Cheon Kang</span><sup>*</sup>, Jaein Kim<sup>*</sup>, Suyeon Shin, Byoung-Tak Zhang</span><br>
    <span class="conf">IROS 2023</span>
</p>
<img src="/img/gvcci_overview.png">
<hr>

<a name="/gst"></a>
<h2 class="pubt">The Dialog Must Go On: Improving Visual Dialog via Generative Self-Training</h2>
<p class="pubd">
    <span class="authors"><span class="u">Gi-Cheon Kang</span>, Sungdong Kim<sup>*</sup>, Jin-Hwa Kim<sup>*</sup>, Donghyun Kwak<sup>*</sup>, Byoung-Tak Zhang</span><br>
    <span class="conf">CVPR 2023</span>
    <span class="links">
        <a target="_blank" href="https://arxiv.org/abs/2205.12502">Paper</a>
        <a target="_blank" href="https://github.com/gicheonkang/gst-visdial">Code</a>
        <a target="_blank" href="https://docs.google.com/viewer?url=https://raw.githubusercontent.com/gicheonkang/gicheonkang.github.io/master/docs/GST-23-slide.pdf">Slides</a>
    </span>
</p>
<img src="/img/gst_overview.gif">
<hr>

<a name="/sglkt"></a>
<h2 class="pubt">Reasoning Visual Dialog with Sparse Graph Learning and Knowledge Transfer</h2>
<p class="pubd">
    <span class="authors"><span class="u">Gi-Cheon Kang</span>, Junseok Park, Hwaran Lee, Byoung-Tak Zhang<sup>*</sup>, Jin-Hwa Kim<sup>*</sup></span><br>
    <span class="conf">EMNLP 2021 Findings</span>
    <span class="links">
        <a target="_blank" href="https://arxiv.org/abs/2004.06698">Paper</a>
        <a target="_blank" href="https://github.com/gicheonkang/sglkt-visdial">Code</a>
        <a target="_blank" href="https://docs.google.com/viewer?url=https://raw.githubusercontent.com/gicheonkang/gicheonkang.github.io/master/docs/SGLKT-21-slide.pdf">Slides</a>
    </span>
</p>
<img src="/img/sglkt_overview.png">
<hr>

<a name="/masn"></a>
<h2 class="pubt">Attend What You Need: Motion-Appearance Synergistic Networks for Video Question Answering</h2>
<p class="pubd">
    <span class="authors">Ahjeong Seo, <span class="u">Gi-Cheon Kang</span>, Joonhan Park, Byoung-Tak Zhang</span><br>
    <span class="conf">ACL 2021</span>
    <span class="links">
        <a target="_blank" href="https://aclanthology.org/2021.acl-long.481">Paper</a>
        <a target="_blank" href="https://github.com/ahjeongseo/MASN-pytorch">Code</a>
    </span>
</p>
<img src="/img/masn_overview.png">
<hr>

<a name="/lpart"></a>
<h2 class="pubt">Label Propagation Adaptive Resonance Theory for Semi-Supervised Continuous Learning</h2>
<p class="pubd">
    <span class="authors">Taehyeong Kim, Injune Hwang, <span class="u">Gi-Cheon Kang</span>, Won-Seok Choi, Hyunseo Kim, Byoung-Tak Zhang</span><br>
    <span class="conf">ICASSP 2020</span>
    <span class="links">
        <a target="_blank" href="https://ieeexplore.ieee.org/document/9054655">Paper</a>
    </span>
</p>
<img src="/img/lpart_overview.png">
<hr>

<a name="/dan"></a>
<h2 class="pubt">Dual Attention Networks for Visual Reference Resolution in Visual Dialog</h2>
<p class="pubd">
    <span class="authors"><span class="u">Gi-Cheon Kang</span>, Jaeseo Lim, Byoung-Tak Zhang</span><br>
    <span class="conf">EMNLP 2019</span><br>
    <span class="conf">3rd Place in VisDial Challenge @ CVPR 2019</span>
    <span class="links">
        <a target="_blank" href="https://www.aclweb.org/anthology/D19-1209">Paper</a>
        <a target="_blank" href="https://github.com/gicheonkang/DAN-VisDial">Code</a>
        <a target="_blank" href="https://docs.google.com/viewer?url=https://raw.githubusercontent.com/gicheonkang/gicheonkang.github.io/master/files/DAN-19-slide.pdf">Slides</a>        
    </span>
</p>
<img src="/img/dan_overview.jpg">
<hr>

# Workshop Papers

<a name="/sfa"></a>
<h2 class="pubt">Improving Robustness to Texture Bias via Shape-focused Augmentation</h2>
<p class="pubd">
    <span class="authors">Sangjun Lee, Inwoo Hwang, <span class="u">Gi-Cheon Kang</span>, Byoung-Tak Zhang</span><br>
    <span class="conf">CVPR 2022 Workshop on Human-centered Intelligent Services: Safety and Trustworthy</span>
    <span class="links">
        <a target="_blank" href="https://openaccess.thecvf.com/content/CVPR2022W/HCIS/papers/Lee_Improving_Robustness_to_Texture_Bias_via_Shape-Focused_Augmentation_CVPRW_2022_paper.pdf">Paper</a>
    </span>
</p>
<img src="/img/sfa_overview.png">
<hr>

<a name="/c3"></a>
<h2 class="pubt">C<sup>3</sup>: Contrastive Learning for Cross-domain Correspondence in Few-shot Image Generation</h2>
<p class="pubd">
    <span class="authors">Hyukgi Lee, <span class="u">Gi-Cheon Kang</span>, Chang-Hoon Jeong, Hanwool Sul, Junseok Park, Byoung-Tak Zhang</span><br>
    <span class="conf">NeurIPS 2021 Workshop on Controllable Generative Modeling in Language and Vision</span>
    <span class="links">
        <a target="_blank" href="https://ctrlgenworkshop.github.io/camready/40/CameraReady/NIPS_Workshop_camera_ready.pdf">Paper</a>
    </span>
</p>
<img src="/img/c3_overview.png">
<hr>


<a name="/services"></a>

# Reviewing

<div class="row">
    <div class="col-xs-12">
        <div class="talkt">
            Neural Information Processing Systems (NeurIPS), 2023
        </div>
        <div class="talkt">
            Annual Meeting of the Association for Computational Linguistics (ACL), 2023
        </div>
        <div class="talkt">
            Conference on Empirical Methods in Natural Language Processing (EMNLP), 2022-2023
        </div>
    </div>
</div>
<hr>

<table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
    <tr>
      <td>
        <br>
        <p align="right"><font size="2">
          <a href="https://abhishekdas.com/">(Courtesy: Abhishek Das)</a>
          <!-- <a href="http://www.cs.berkeley.edu/~barron/"> this website</a> -->
          </font>
        </p>
      </td>
    </tr>
</table>

<script src="/js/jquery.min.js"></script>
<script type="text/javascript">
    $('ul:gt(0) li:gt(12)').hide();
    $('#read-more-button > a').click(function() {
        $('ul:gt(0) li:gt(12)').show();
        $('#read-more-button').hide();
    });
</script>

---

[1]: http://en.snu.ac.kr
[2]: https://aiis.snu.ac.kr/eng/
[3]: https://bi.snu.ac.kr/~btzhang/
[4]: https://gicheonkang.com
[5]: http://www.ajou.ac.kr/en/